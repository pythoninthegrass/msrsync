#!/usr/bin/env python

# Copyright 2017 Jean-Baptiste Denis <jbd@jbdenis.net>

# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License version 3, as published
# by the Free Software Foundation.
#
# This file includes a copy of the BSD licensed options.py file from the bup project
# See https://github.com/bup/bup/blob/master/lib/bup/options.py

VERSION = '20250809'

DEFAULT_RSYNC_OPTIONS = "-aS --numeric-ids"

MSRSYNC_OPTSPEC = f"""
msrsync [options] [--rsync "rsync-options-string"] SRCDIR [SRCDIR2...] DESTDIR
msrsync --selftest
--
 msrsync options:
p,processes=   number of rsync processes to use [1]
f,files=       limit buckets to <files> files number [1000]
s,size=        limit partitions to BYTES size (1024 suffixes: K, M, G, T, P, E, Z, Y) [1G]
b,buckets=     where to put the buckets files (default: auto temporary directory)
k,keep         do not remove buckets directory at the end
j,show         show bucket directory
P,progress     show progress
stats          show additional stats
d,dry-run      do not run rsync processes
v,version      print version
 rsync options:
r,rsync=       MUST be last option. rsync options as a quoted string ["{DEFAULT_RSYNC_OPTIONS}"]. The "--from0 --files-from=... --quiet --verbose --stats --log-file=..." options will ALWAYS be added, no matter what. Be aware that this will affect all rsync *from/filter files if you want to use them. See rsync(1) manpage for details.
 self-test options:
t,selftest     run the integrated unit and functional tests
"""

import contextlib
import datetime
import getopt
import gzip
import itertools
import multiprocessing
import os
import platform
import random
import re
import shlex
import shutil
import signal
import struct
import subprocess
import sys
import tempfile
import textwrap
import threading
import time
import timeit
import traceback
import unittest
from multiprocessing.managers import SyncManager

RSYNC_EXE = None
EOPTION_PARSER = 97
EBUCKET_DIR_NOEXIST = 11
EBUCKET_DIR_PERMS = 12
EBUCKET_DIR_OSERROR = 12
EBUCKET_FILE_CREATE = 13
EBIN_NOTFOUND = 14
ESRC_NOT_DIR = 15
ESRC_NO_ACCESS = 16
EDEST_NO_ACCESS = 17
EDEST_NOT_DIR = 18
ERSYNC_OPTIONS_CHECK = 19
ERSYNC_TOO_LONG = 20
ERSYNC_JOB = 21
ERSYNC_OK = 22
EDEST_IS_FILE = 23
EDEST_CREATE = 24
ENEED_ROOT = 25
EMSRSYNC_INTERRUPTED = 26
TYPE_RSYNC = 0
TYPE_RSYNC_SENTINEL = 1
MSG_STDERR = 10
MSG_STDOUT = 11
MSG_PROGRESS = 12
G_MESSAGES_QUEUE = None

"""Command-line options parser.
With the help of an options spec string, easily parse command-line options.

An options spec is made up of two parts, separated by a line with two dashes.
The first part is the synopsis of the command and the second one specifies
options, one per line.

Each non-empty line in the synopsis gives a set of options that can be used
together.

Option flags must be at the begining of the line and multiple flags are
separated by commas. Usually, options have a short, one character flag, and a
longer one, but the short one can be omitted.

Long option flags are used as the option's key for the OptDict produced when
parsing options.

When the flag definition is ended with an equal sign, the option takes
one string as an argument, and that string will be converted to an
integer when possible. Otherwise, the option does not take an argument
and corresponds to a boolean flag that is true when the option is
given on the command line.

The option's description is found at the right of its flags definition, after
one or more spaces. The description ends at the end of the line. If the
description contains text enclosed in square brackets, the enclosed text will
be used as the option's default value.

Options can be put in different groups. Options in the same group must be on
consecutive lines. Groups are formed by inserting a line that begins with a
space. The text on that line will be output after an empty line.
"""


def _invert(v, invert):
    if invert:
        return not v
    return v


def _remove_negative_kv(k, v):
    if k.startswith('no-') or k.startswith('no_'):
        return k[3:], not v
    return k, v


class OptDict:
    """Dictionary that exposes keys as attributes.

    Keys can be set or accessed with a "no-" or "no_" prefix to negate the
    value.
    """

    def __init__(self, aliases):
        self._opts = {}
        self._aliases = aliases

    def _unalias(self, k):
        k, reinvert = _remove_negative_kv(k, False)
        k, invert = self._aliases[k]
        return k, invert ^ reinvert

    def __setitem__(self, k, v):
        k, invert = self._unalias(k)
        self._opts[k] = _invert(v, invert)

    def __getitem__(self, k):
        k, invert = self._unalias(k)
        return _invert(self._opts[k], invert)

    def __getattr__(self, k):
        if k.startswith('_'):
            raise AttributeError(k)
        return self[k]


def _default_onabort(msg):
    sys.exit(97)


def _intify(v):
    try:
        vv = int(v or '')
        if str(vv) == v:
            return vv
    except ValueError:
        pass
    return v


def _atoi(v):
    try:
        return int(v or 0)
    except ValueError:
        return 0


def _tty_width():
    if not hasattr(sys.stderr, "fileno"):
        return _atoi(os.environ.get('WIDTH')) or 70
    s = struct.pack("HHHH", 0, 0, 0, 0)
    try:
        import fcntl
        import termios

        s = fcntl.ioctl(sys.stderr.fileno(), termios.TIOCGWINSZ, s)
    except (OSError, ImportError):
        return _atoi(os.environ.get('WIDTH')) or 70
    (ysize, xsize, ypix, xpix) = struct.unpack('HHHH', s)
    return xsize or 70


class Options:
    """Option parser.
    When constructed, a string called an option spec must be given. It
    specifies the synopsis and option flags and their description.  For more
    information about option specs, see the docstring at the top of this file.

    Two optional arguments specify an alternative parsing function and an
    alternative behaviour on abort (after having output the usage string).

    By default, the parser function is getopt.gnu_getopt, and the abort
    behaviour is to exit the program.
    """

    def __init__(self, optspec, optfunc=getopt.gnu_getopt, onabort=_default_onabort):
        self.optspec = optspec
        self._onabort = onabort
        self.optfunc = optfunc
        self._aliases = {}
        self._shortopts = 'h?'
        self._longopts = ['help', 'usage']
        self._hasparms = {}
        self._defaults = {}
        self._usagestr = self._gen_usage()

    def _gen_usage(self):
        out = []
        lines = self.optspec.strip().split('\n')
        lines.reverse()
        first_syn = True
        while lines:
            line = lines.pop()
            if line == '--':
                break
            out.append(f"{first_syn and 'usage' or '   or'}: {line}\n")
            first_syn = False
        out.append('\n')
        last_was_option = False
        while lines:
            line = lines.pop()
            if line.startswith(' '):
                out.append(f"{last_was_option and '\n' or ''}{line.lstrip()}\n")
                last_was_option = False
            elif line:
                (flags, extra) = (line + ' ').split(' ', 1)
                extra = extra.strip()
                if flags.endswith('='):
                    flags = flags[:-1]
                    has_parm = 1
                else:
                    has_parm = 0
                g = re.search(r'\[([^\]]*)\]$', extra)
                defval = _intify(g.group(1)) if g else None
                flagl = flags.split(',')
                flagl_nice = []
                flag_main, invert_main = _remove_negative_kv(flagl[0], False)
                self._defaults[flag_main] = _invert(defval, invert_main)
                for _f in flagl:
                    f, invert = _remove_negative_kv(_f, 0)
                    self._aliases[f] = (flag_main, invert_main ^ invert)
                    self._hasparms[f] = has_parm
                    if f == '#':
                        self._shortopts += '0123456789'
                        flagl_nice.append('-#')
                    elif len(f) == 1:
                        self._shortopts += f + (has_parm and ':' or '')
                        flagl_nice.append('-' + f)
                    else:
                        f_nice = re.sub(r'\W', '_', f)
                        self._aliases[f_nice] = (flag_main, invert_main ^ invert)
                        self._longopts.append(f + (has_parm and '=' or ''))
                        self._longopts.append('no-' + f)
                        flagl_nice.append('--' + _f)
                flags_nice = ', '.join(flagl_nice)
                if has_parm:
                    flags_nice += ' ...'
                prefix = f'    {flags_nice:<20}  '
                argtext = '\n'.join(textwrap.wrap(extra, width=_tty_width(), initial_indent=prefix, subsequent_indent=' ' * 28))
                out.append(argtext + '\n')
                last_was_option = True
            else:
                out.append('\n')
                last_was_option = False
        return ''.join(out).rstrip() + '\n'

    def usage(self, msg=""):
        """Print usage string to stderr and abort."""
        sys.stderr.write(self._usagestr)
        if msg:
            sys.stderr.write(msg)
        e = self._onabort and self._onabort(msg) or None
        if e:
            raise e

    def fatal(self, msg):
        """Print an error message to stderr and abort with usage string."""
        msg = f'\nerror: {msg}\n'
        return self.usage(msg)

    def parse(self, args):
        """Parse a list of arguments and return (options, flags, extra).

        In the returned tuple, "options" is an OptDict with known options,
        "flags" is a list of option flags that were used on the command-line,
        and "extra" is a list of positional arguments.
        """
        try:
            (flags, extra) = self.optfunc(args, self._shortopts, self._longopts)
        except getopt.GetoptError as e:
            self.fatal(e)

        opt = OptDict(aliases=self._aliases)

        for k, v in self._defaults.items():
            opt[k] = v

        for k, v in flags:
            k = k.lstrip('-')
            if k in ('h', '?', 'help', 'usage'):
                self.usage()
            if self._aliases.get('#') and k in ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9'):
                v = int(k)  # guaranteed to be exactly one digit
                k, invert = self._aliases['#']
                opt['#'] = v
            else:
                k, invert = opt._unalias(k)
                if not self._hasparms[k]:
                    assert v == ''
                    v = (opt._opts.get(k) or 0) + 1
                else:
                    v = _intify(v)
            opt[k] = _invert(v, invert)
        return (opt, flags, extra)


def print_message(message, output=MSG_STDOUT):
    """
    Add message to the message queue
    """
    G_MESSAGES_QUEUE.put({"type": output, "message": message})


def print_update(data):
    """
    Print 'data' on the same line as before
    """
    sys.stdout.write("\r\x1b[K" + data.__str__())
    sys.stdout.flush()


class BucketError(RuntimeError):
    """
    Exception for bucket related error
    """

    pass


def get_human_size(num, power="B"):
    """
    Stolen from the ps_mem.py project for nice size output :)
    """
    powers = ["B", "K", "M", "G", "T", "P", "E", "Z", "Y"]
    while num >= 1000:  # 4 digits
        num /= 1024.0
        power = powers[powers.index(power) + 1]
    return f"{num:.1f} {power}"


def human_size(value):
    """
    parse the provided human size (with multiples K, M, G, T, E, P, Z, Y)
    and return bytes
    """

    if value.isdigit():
        return int(value)

    if not value[:-1].isdigit():
        return None

    m2s = {
        'K': 1024,
        'M': 1024 * 1024,
        'G': 1024 * 1024 * 1024,
        'T': 1024 * 1024 * 1024 * 1024,
        'P': 1024 * 1024 * 1024 * 1024 * 1024,
        'E': 1024 * 1024 * 1024 * 1024 * 1024 * 1024,
        'Z': 1024 * 1024 * 1024 * 1024 * 1024 * 1024 * 1024,
        'Y': 1024 * 1024 * 1024 * 1024 * 1024 * 1024 * 1024 * 1024,
    }

    size = int(value[:-1])
    multiple = value[-1]

    if multiple not in list(m2s.keys()):
        return None

    return size * m2s[multiple]


def crawl(path, relative=False):
    """
    Simple generator around os.walk that will
    yield (size, fullpath) tuple for each file or link
    underneath path.

    If relative is True, the path will be relative to path, without
    any leading ./ or /. For exemple, crawl("/home/jbdenis/Code", relative=True)
    will yield (42, "toto") for "/home/jbdenis/Code/toto" file
    """

    def onerror(oserror):
        """
        helper
        """
        print_message(f"msrsync crawl: {oserror}", MSG_STDERR)

    root_size = len(path) if relative else 0

    for root, dirs, files in os.walk(path, onerror=onerror):
        # we want empty dir to be listed in bucket
        if len(dirs) == 0 and len(files) == 0:
            rpath = root[root_size:]
            try:
                yield os.lstat(root).st_size, rpath
            except OSError as err:
                print_message(f"msrsync crawl: {err}", MSG_STDERR)
                continue

        dir_links = [d for d in dirs if os.path.islink(os.path.join(root, d))]

        for name in itertools.chain(files, dir_links):
            fullpath = os.path.join(root, name)
            try:
                size = os.lstat(fullpath).st_size
            except OSError as err:
                print_message(f"msrsync crawl: {err}", MSG_STDERR)
                continue

            rpath = fullpath[root_size:]
            yield size, rpath


def buckets(path, filesnr, size):
    """
    Split files underneath path in buckets less than <size> bytes in total
    or containing <filesnr> files maximum.
    """
    bucket_files_nr = 0
    bucket_size = 0
    bucket = list()
    base = os.path.split(path)[1]

    for fsize, rpath in crawl(path, relative=True):
        bucket.append(os.path.join(base, rpath.lstrip(os.sep)))
        bucket_files_nr += 1
        bucket_size += fsize

        if bucket_size >= size or bucket_files_nr >= filesnr:
            yield (bucket_files_nr, bucket_size, bucket)
            bucket_size = 0
            bucket_files_nr = 0
            bucket = list()

    if bucket_files_nr > 0:
        yield (bucket_files_nr, bucket_size, bucket)


def _valid_rsync_options(options, rsync_opts):
    """
    Check for weird stuff in rsync options
    """
    rsync_args = rsync_opts.split()
    for opt in rsync_args:
        if opt.startswith("--delete"):
            options.fatal("Cannot use --delete option type with msrsync. It would lead to disaster :)")


def parse_cmdline(cmdline_argv):
    """
    command line parsing of msrsync using bup/options.py
    See https://github.com/bup/bup/blob/master/lib/bup/options.py
    """
    options = Options(MSRSYNC_OPTSPEC)

    if "-r" in cmdline_argv:
        idx = cmdline_argv.index("-r")
        cmdline_argv[idx] = "--rsync"

    if "--rsync" in cmdline_argv:
        idx = cmdline_argv.index("--rsync")
        (opt, _, extra) = options.parse(cmdline_argv[1:idx])
        if len(cmdline_argv[idx:]) < 4:
            options.fatal('You must provide a source, a destination and eventually rsync options with --rsync')
        opt.rsync = opt.r = cmdline_argv[idx + 1]
        _valid_rsync_options(options, opt.rsync)
        srcs, dest = cmdline_argv[idx + 2 : -1], cmdline_argv[-1]
    else:
        (opt, _, extra) = options.parse(cmdline_argv[1:])
        if opt.selftest or opt.version:
            return opt, [], ""
        opt.rsync = opt.r = DEFAULT_RSYNC_OPTIONS
        if not extra or len(extra) < 2:
            options.fatal('You must provide a source and a destination')
        srcs, dest = extra[:-1], extra[-1]

    size = human_size(str(opt.size))
    if not size:
        options.fatal(f"'{opt.size}' does not look like a valid size value")
    try:
        opt.files = opt.f = int(opt.f)
    except ValueError:
        options.fatal(f"'{opt.f}' does not look like a valid files number value")
    opt.size = opt.s = size
    opt.compress = False

    return opt, srcs, dest


def rmtree_onerror(func, path, exc_info):
    """
    Error handler for shutil.rmtree.
    """
    print("Error removing", path, file=sys.stderr)


def write_bucket(filename, bucket, compress=False):
    """
    Dump bucket filenames in a optionally compressed file
    """
    try:
        fileno, path = filename
        if not compress:
            with os.fdopen(fileno, 'wb') as bfile:
                for entry in bucket:
                    # Encode with surrogateescape to handle invalid UTF-8 sequences
                    bfile.write((entry + '\0').encode('utf-8', 'surrogateescape'))
        else:
            os.close(fileno)
            with gzip.open(path, 'wb') as bfile:
                for entry in bucket:
                    bfile.write(entry.encode('utf-8', 'surrogateescape'))
    except OSError as err:
        raise BucketError(f"Cannot write bucket file {path}: {err}") from err


def consume_queue(jobs_queue):
    """
    Simple helper around a shared queue
    """
    while True:
        item = jobs_queue.get()
        if item is StopIteration:
            return
        yield item


# http://stackoverflow.com/questions/1191374/subprocess-with-timeout
def kill_proc(proc, timeout):
    """helper function for run"""
    timeout["value"] = True
    with contextlib.suppress(OSError, ProcessLookupError):
        # Try to kill the entire process group first
        try:
            os.killpg(proc.pid, signal.SIGTERM)
            time.sleep(0.1)
            os.killpg(proc.pid, signal.SIGKILL)
        except (OSError, ProcessLookupError):
            # Fallback to single process termination
            proc.terminate()
            time.sleep(0.1)
            proc.kill()


# http://stackoverflow.com/questions/1191374/subprocess-with-timeout
def run(cmd, capture_stdout=False, capture_stderr=False, timeout_sec=sys.maxsize):
    """run function with a timeout"""
    return run_tracked(cmd, None, capture_stdout, capture_stderr, timeout_sec)


def run_tracked(cmd, proc_tracker, capture_stdout=False, capture_stderr=False, timeout_sec=sys.maxsize):
    """run function with a timeout and process tracking"""
    try:
        stdout_p = subprocess.PIPE if capture_stdout else None
        stderr_p = subprocess.PIPE if capture_stderr else None
        # Create subprocess in new process group to ensure proper signal handling
        proc = subprocess.Popen(shlex.split(cmd), stdout=stdout_p, stderr=stderr_p, preexec_fn=os.setpgrp)

        # Track process if tracker provided
        if proc_tracker is not None:
            proc_tracker["proc"] = proc

        timeout = {"value": False}
        timer = threading.Timer(timeout_sec, kill_proc, [proc, timeout])
        starttime = time.time()
        timer.start()
        stdout, stderr = proc.communicate()
        if stdout is None:
            stdout = ""
        if stderr is None:
            stderr = ""
        timer.cancel()
        elapsed = time.time() - starttime

        # Clear process tracker when done
        if proc_tracker is not None:
            proc_tracker["proc"] = None

    except OSError as err:
        return -1, "", f"Cannot launch {cmd}: {err}", False, 0
    except KeyboardInterrupt:
        if 'timer' in vars():
            timer.cancel()
        if proc:
            if proc.stdout:
                proc.stdout.close()
            if proc.stderr:
                proc.stderr.close()
            # Terminate the entire process group
            try:
                os.killpg(proc.pid, signal.SIGTERM)
                time.sleep(0.1)
                os.killpg(proc.pid, signal.SIGKILL)
            except (OSError, ProcessLookupError):
                # Fallback to single process termination
                proc.terminate()
                time.sleep(0.1)
                proc.kill()
            proc.wait()
        # Clear process tracker on interrupt
        if proc_tracker is not None:
            proc_tracker["proc"] = None
        return EMSRSYNC_INTERRUPTED, "", "Interrupted", False, 0

    return (
        proc.returncode,
        stdout.decode('utf-8', 'replace') if stdout else "",
        stderr.decode('utf-8', 'replace') if stderr else "",
        timeout["value"],
        elapsed,
    )


# http://stackoverflow.com/a/377028
def which(program):
    """
    Python implementation of the which command
    """

    def is_exe(fpath):
        """helper"""
        return os.path.isfile(fpath) and os.access(fpath, os.X_OK)

    fpath, _ = os.path.split(program)
    if fpath:
        if is_exe(program):
            return program
    else:
        for path in os.environ["PATH"].split(os.pathsep):
            exe_file = os.path.join(path, program)
            if is_exe(exe_file):
                return exe_file

    return None


def _check_rsync_options(options):
    """
    Build a command line given the rsync options string
    and try to execute it on empty directory
    """
    rsync_cmd = None
    try:
        src = tempfile.mkdtemp()
        dst = tempfile.mkdtemp()
        rsync_log_fd, rsync_log = tempfile.mkstemp()
        rsync_cmd = f"{RSYNC_EXE} {options + ' --quiet --stats --verbose --from0 --log-file ' + rsync_log} {src + os.sep} {dst}"
        ret, _, stderr, timeout, _ = run(rsync_cmd, timeout_sec=60)
        if timeout:
            print(f'''Error during rsync options check command "{rsync_cmd}": took more than 60 seconds !''', file=sys.stderr)
            sys.exit(ERSYNC_OPTIONS_CHECK)
        elif ret != 0:
            print(f'''Error during rsync options check command "{rsync_cmd}": {2 * os.linesep + stderr}''', file=sys.stderr)
            sys.exit(ERSYNC_OPTIONS_CHECK)
    except OSError as err:
        if rsync_cmd:
            print(f'''Error during rsync options check command "{rsync_cmd}": {2 * os.linesep + err}''', file=sys.stderr)
        else:
            print(f'''Error during rsync options check ("{options}"): {2 * os.linesep + err}''', file=sys.stderr)
        sys.exit(ERSYNC_OPTIONS_CHECK)
    finally:
        try:
            os.rmdir(src)
            os.rmdir(dst)
            os.close(rsync_log_fd)
            os.remove(rsync_log)
        except OSError:
            pass


def run_rsync(files_from, rsync_opts, src, dest, timeout=3600 * 24 * 7):
    """
    Perform rsync using the --files-from option
    """
    return run_rsync_tracked(files_from, rsync_opts, src, dest, None, timeout)


def run_rsync_tracked(files_from, rsync_opts, src, dest, proc_tracker, timeout=3600 * 24 * 7):
    """
    Perform rsync using the --files-from option with process tracking
    """
    rsync_log = files_from + '.log'
    rsync_cmd = f'{RSYNC_EXE} {rsync_opts} --quiet --verbose --stats --from0 --files-from={files_from} --log-file={rsync_log} "{src}" "{dest}"'
    rsync_result = dict()
    rsync_result["rcode"] = -1
    rsync_result["msg"] = None
    rsync_result["cmdline"] = rsync_cmd
    rsync_result["log"] = rsync_log

    try:
        ret, _, _, timeout, elapsed = run_tracked(rsync_cmd, proc_tracker, timeout_sec=timeout)

        rsync_result["rcode"] = ret
        rsync_result["elapsed"] = elapsed

        if timeout:
            rsync_result["errcode"] = ERSYNC_TOO_LONG
        elif ret != 0:
            rsync_result["errcode"] = ERSYNC_JOB

    except OSError as err:
        rsync_result["errcode"] = ERSYNC_JOB
        rsync_result["msg"] = str(err)

    return rsync_result


def rsync_worker(jobs_queue, monitor_queue, options, dest, rsync_exe):
    """
    The queue will contains filenames of file to handle by individual rsync processes
    """

    # Track current rsync process for signal handling
    current_rsync_proc = {"proc": None}

    def worker_signal_handler(signum, frame):
        """Handle signals in worker process by terminating any running rsync"""
        if current_rsync_proc["proc"]:
            try:
                os.killpg(current_rsync_proc["proc"].pid, signal.SIGTERM)
                time.sleep(0.1)
                os.killpg(current_rsync_proc["proc"].pid, signal.SIGKILL)
            except (OSError, ProcessLookupError):
                # Fallback to single process termination
                current_rsync_proc["proc"].terminate()
                current_rsync_proc["proc"].kill()
        # Exit gracefully without raising KeyboardInterrupt to avoid traceback
        sys.exit(1)

    # Set up signal handlers in worker process
    signal.signal(signal.SIGINT, worker_signal_handler)
    signal.signal(signal.SIGTERM, worker_signal_handler)

    # Set the global RSYNC_EXE for this worker process
    global RSYNC_EXE
    RSYNC_EXE = rsync_exe

    try:
        for src, files_from, bucket_files_nr, bucket_size in consume_queue(jobs_queue):
            if not options.dry_run:
                rsync_result = run_rsync_tracked(files_from, options.rsync, src, dest, current_rsync_proc)
            else:
                rsync_result = dict(rcode=0, elapsed=0, errcode=0, msg='')
            rsync_mon_result = {
                "type": TYPE_RSYNC,
                "rsync_result": rsync_result,
                "size": bucket_size,
                "files_nr": bucket_files_nr,
                "jq_size": jobs_queue.qsize(),
            }
            monitor_queue.put(rsync_mon_result)
    except (KeyboardInterrupt, SystemExit):
        pass
    finally:
        try:
            jobs_queue.put(StopIteration)
            # we insert a sentinel value to inform the monitor this process fnished
            monitor_queue.put({"type": TYPE_RSYNC_SENTINEL, "pid": os.getpid()})
        except (OSError, BrokenPipeError, ConnectionResetError, EOFError):
            # Queue manager may have been shut down during signal handling
            pass


def handle_rsync_error_result(rsync_result, messages_queue):
    """
    Helper
    """
    msg = rsync_result["msg"] or ''

    try:
        if rsync_result["errcode"] == ERSYNC_TOO_LONG:
            messages_queue.put(
                {
                    "type": MSG_STDERR,
                    "message": f"rsync command took too long and has been killed (see '{rsync_result['log']}' rsync log file): {msg}\n{rsync_result['cmdline']}",
                }
            )
        elif rsync_result["errcode"] == ERSYNC_JOB:
            messages_queue.put(
                {
                    "type": MSG_STDERR,
                    "message": f"errors during rsync command (see '{rsync_result['log']}' rsync log file): {msg}\n{rsync_result['cmdline']}",
                }
            )
        else:
            messages_queue.put({"type": MSG_STDERR, "message": f"unknown rsync_result status: {rsync_result}"})
    except (OSError, BrokenPipeError, ConnectionResetError, EOFError):
        pass


def rsync_monitor_worker(
    monitor_queue, nb_rsync_processes, total_size, total_files_nr, crawl_time, total_time, options, messages_queue
):
    """
    The monitor queue contains messages from the rsync workers
    """

    # Set up signal handlers in worker process
    signal.signal(signal.SIGINT, signal.SIG_DFL)
    signal.signal(signal.SIGTERM, signal.SIG_DFL)
    current_size = 0
    current_files_nr = 0
    current_elapsed = 0
    rsync_runtime = 0
    rsync_workers_stops = 0
    buckets_nr = 0
    rsync_errors = 0
    entries_per_second = 0
    bytes_per_second = 0

    try:
        start = timeit.default_timer()
        for result in consume_queue(monitor_queue):
            if result["type"] == TYPE_RSYNC_SENTINEL:
                # not needed, but we keep it for now
                rsync_workers_stops += 1
                continue
            if result["type"] != TYPE_RSYNC:
                with contextlib.suppress(OSError, BrokenPipeError, ConnectionResetError, EOFError):
                    messages_queue.put(
                        {
                            "type": MSG_STDERR,
                            "message": f"rsync_monitor_worker process received an incompatile type message: {result}",
                        }
                    )
                continue

            rsync_result = result["rsync_result"]

            if rsync_result["rcode"] != 0:
                rsync_errors += 1
                handle_rsync_error_result(rsync_result, messages_queue)
                continue

            buckets_nr += 1
            current_size += result["size"]
            current_files_nr += result["files_nr"]
            rsync_runtime += result["rsync_result"]["elapsed"]
            current_elapsed = timeit.default_timer() - start

            bytes_per_second = current_size / current_elapsed if current_elapsed > 0 else 0

            entries_per_second = current_files_nr / current_elapsed if current_elapsed > 0 else 0

            if options.progress:
                with contextlib.suppress(OSError, BrokenPipeError, ConnectionResetError, EOFError):
                    messages_queue.put(
                        {
                            "type": MSG_PROGRESS,
                            "message": f"[{current_files_nr}/{total_files_nr.value} entries] [{get_human_size(current_size)}/{get_human_size(total_size.value)} transferred] [{entries_per_second} entries/s] [{get_human_size(bytes_per_second)}/s bw] [monq {monitor_queue.qsize()}] [jq {result['jq_size']}]",
                        }
                    )

        if rsync_errors > 0:
            with contextlib.suppress(OSError, BrokenPipeError, ConnectionResetError, EOFError):
                messages_queue.put(
                    {
                        "type": MSG_STDERR,
                        "message": "\nmsrsync error: somes files/attr were not transferred (see previous errors)",
                    }
                )

        stats = dict()
        stats["errors"] = rsync_errors
        stats["total_size"] = total_size.value
        stats["total_entries"] = total_files_nr.value
        stats["buckets_nr"] = buckets_nr
        stats["bytes_per_second"] = bytes_per_second
        stats["entries_per_second"] = entries_per_second
        stats["rsync_workers"] = nb_rsync_processes
        stats["rsync_runtime"] = rsync_runtime
        stats["crawl_time"] = crawl_time.value
        stats["total_time"] = total_time.value

        with contextlib.suppress(OSError, BrokenPipeError, ConnectionResetError, EOFError):
            monitor_queue.put(stats)

    except (KeyboardInterrupt, SystemExit):
        pass


def messages_worker(options, messages_queue):
    """
    This queue will contains messages to be print of the screen
    """

    # Set up signal handlers in worker process
    signal.signal(signal.SIGINT, signal.SIG_DFL)
    signal.signal(signal.SIGTERM, signal.SIG_DFL)

    last_msg_type = cur_msg_type = None
    try:
        for result in consume_queue(messages_queue):
            newline = os.linesep if last_msg_type == MSG_PROGRESS else ''

            cur_msg_type = result["type"]

            if cur_msg_type == MSG_PROGRESS:
                print_update(result["message"])
            elif cur_msg_type == MSG_STDOUT:
                print(newline + result["message"], file=sys.stdout)
            elif cur_msg_type == MSG_STDERR:
                print(newline + result["message"], file=sys.stderr)
            else:
                print(newline + f"Unknown message type '{cur_msg_type}': {result}", file=sys.stderr)
            last_msg_type = cur_msg_type

    except (KeyboardInterrupt, SystemExit):
        pass
    finally:
        if last_msg_type == MSG_PROGRESS:
            print('', file=sys.stdout)


def start_rsync_workers(jobs_queue, monitor_queue, options, dest):
    """
    Helper to start rsync processes
    """
    processes = []
    for _ in range(options.processes):
        processes.append(multiprocessing.Process(target=rsync_worker, args=(jobs_queue, monitor_queue, options, dest, RSYNC_EXE)))
        processes[-1].start()
    return processes


def start_rsync_monitor_worker(
    monitor_queue, nb_rsync_processes, total_size, total_files_nr, crawl_time, total_time, options, messages_queue
):
    """
    Helper to start rsync monitor process
    """
    proc = multiprocessing.Process(
        target=rsync_monitor_worker,
        args=(monitor_queue, nb_rsync_processes, total_size, total_files_nr, crawl_time, total_time, options, messages_queue),
    )
    proc.start()
    return proc


def start_messages_worker(options, messages_queue):
    """
    Helper to start messages process
    """
    proc = multiprocessing.Process(target=messages_worker, args=(options, messages_queue))
    proc.start()
    return proc


def multiprocess_mgr_init():
    """
    Explicit initializer for SyncManager in msrsync function
    """
    signal.signal(signal.SIGINT, signal.SIG_IGN)


def show_stats(msrsync_stat):
    """
    Show the stats from msrsync run
    """
    s = msrsync_stat
    status = "SUCCESS" if s["errors"] == 0 else f"FAILURE, {s['errors']} rsync processe(s) had errors"

    print("Status:", status)
    print("Working directory:", os.getcwd())
    print("Command line:", " ".join(sys.argv))
    print(f"Total size: {get_human_size(s['total_size'])}")
    print(f"Total entries: {s['total_entries']}")
    buckets_nr = s["buckets_nr"]
    print(f"Buckets number: {buckets_nr}")
    if buckets_nr > 0:
        print(f"Mean entries per bucket: {int((s['total_entries'] * 1.0) / buckets_nr)}")
        print(f"Mean size per bucket: {get_human_size((s['total_size'] * 1.0) / buckets_nr)}")

    print(f"Entries per second: {s['entries_per_second']:.0f}")
    print(f"Speed: {get_human_size(s['bytes_per_second'])}/s")
    print(f"Rsync workers: {s['rsync_workers']}")
    print(f"Total rsync's processes ({buckets_nr}) cumulative runtime: {s['rsync_runtime']:.1f}s")
    print(f"Crawl time: {s['crawl_time']:.1f}s ({100 * s['crawl_time'] / s['total_time']:.1f}% of total runtime)")
    print(f"Total time: {s['total_time']:.1f}s")


def msrsync(options, srcs, dest):
    """
    multi-stream rsync reusable function
    It will copy srcs directories to dest honoring the options structure
    """
    global G_MESSAGES_QUEUE
    try:
        if not options.buckets:
            options.buckets = tempfile.mkdtemp(prefix="msrsync-")
        else:
            if not os.path.exists(options.buckets):
                print(options.buckets, "bucket directory does not exist.", file=sys.stderr)
                sys.exit(EBUCKET_DIR_NOEXIST)
            if not os.access(options.buckets, os.W_OK):
                print(options.buckets, "bucket directory is not writable.", file=sys.stderr)
                sys.exit(EBUCKET_DIR_PERMS)
            options.buckets = tempfile.mkdtemp(prefix="msrsync-", dir=options.buckets)
    except OSError as err:
        print(f'''Error with bucket directory creation: "{err}"''', file=sys.stderr)
        sys.exit(EBUCKET_DIR_OSERROR)

    if options.show:
        print("buckets dir is", options.buckets)

    manager = SyncManager()
    manager.start()

    # Set up signal handlers to ensure clean shutdown
    def signal_handler(signum, frame):
        """Handle interrupt signals by raising KeyboardInterrupt"""
        raise KeyboardInterrupt()

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    total_size = manager.Value('i', 0)
    total_files_nr = manager.Value('i', 0)
    crawl_time = manager.Value('f', 0)
    total_time = manager.Value('f', 0)

    monitor_queue = manager.Queue()
    jobs_queue = manager.Queue()
    G_MESSAGES_QUEUE = manager.Queue()

    rsync_workers_procs = start_rsync_workers(jobs_queue, monitor_queue, options, dest)
    rsync_monitor_worker_proc = start_rsync_monitor_worker(
        monitor_queue, options.processes, total_size, total_files_nr, crawl_time, total_time, options, G_MESSAGES_QUEUE
    )
    messages_worker_proc = start_messages_worker(options, G_MESSAGES_QUEUE)

    crawl_start = timeit.default_timer()

    try:
        total_size.value = 0
        bucket_nr = 0
        for src in srcs:
            head, tail = os.path.split(src)
            src_base = os.getcwd() if head == '' else head

            for bucket_files_nr, bucket_size, bucket in buckets(src, options.files, options.s):
                total_size.value += bucket_size
                total_files_nr.value += bucket_files_nr
                bucket.sort()
                d1s = str(bucket_nr / 1024).zfill(8)
                try:
                    tdir = os.path.join(options.buckets, d1s[:4], d1s[4:])
                    if not os.path.exists(tdir):
                        os.makedirs(tdir)
                    fileno, filename = tempfile.mkstemp(dir=tdir)
                except OSError as err:
                    print_message(f'msrsync scan: cannot create temporary bucket file: "{err}"', MSG_STDERR)
                    continue
                write_bucket((fileno, filename), bucket, options.compress)
                bucket_nr += 1
                jobs_queue.put((src_base, filename, bucket_files_nr, bucket_size))

        crawl_time.value = timeit.default_timer() - crawl_start

        jobs_queue.put(StopIteration)

        for worker in rsync_workers_procs:
            worker.join()

        total_time.value = timeit.default_timer() - crawl_start

        monitor_queue.put(StopIteration)
        rsync_monitor_worker_proc.join()

        G_MESSAGES_QUEUE.put(StopIteration)
        messages_worker_proc.join()

        run_stats = monitor_queue.get()
        if options.stats:
            show_stats(run_stats)

        return run_stats["errors"]

    except (KeyboardInterrupt, SystemExit):
        print("\nInterrupted! Terminating processes...", file=sys.stderr)

        # Flush output streams before redirecting
        sys.stdout.flush()
        sys.stderr.flush()

        # Temporarily redirect stderr to suppress rsync error messages during cleanup
        original_stderr = sys.stderr

        with open(os.devnull, 'w') as devnull:
            sys.stderr = devnull

            try:
                # Terminate all worker processes
                for worker in rsync_workers_procs:
                    worker.terminate()

                # Terminate monitor and message processes
                rsync_monitor_worker_proc.terminate()
                messages_worker_proc.terminate()

                # Wait for all processes to finish with timeout
                for worker in rsync_workers_procs:
                    worker.join(timeout=2.0)
                    if worker.is_alive():
                        worker.kill()

                rsync_monitor_worker_proc.join(timeout=2.0)
                if rsync_monitor_worker_proc.is_alive():
                    rsync_monitor_worker_proc.kill()

                messages_worker_proc.join(timeout=2.0)
                if messages_worker_proc.is_alive():
                    messages_worker_proc.kill()
            finally:
                # Restore stderr
                sys.stderr = original_stderr

        # Return interrupted error code
        return EMSRSYNC_INTERRUPTED
    except BucketError as err:
        print(err, file=sys.stderr)
    except Exception:
        print("Uncaught exception:" + os.linesep + traceback.format_exc(), file=sys.stderr)
    finally:
        manager.shutdown()
        if options.buckets is not None and not options.keep:
            shutil.rmtree(options.buckets, onerror=rmtree_onerror)


def _check_executables():
    """
    Get the full path of somes binaries
    """
    global RSYNC_EXE

    exes = ["rsync"]
    paths = dict()

    for exe in exes:
        prog = which(exe)
        if not prog:
            print(f"Cannot find '{exe}' executable in PATH.", file=sys.stderr)
            sys.exit(EBIN_NOTFOUND)
        paths[exe] = prog

    RSYNC_EXE = paths["rsync"]


def _check_srcs_dest(srcs, dest):
    """
    Check that the supplied arguments are valid
    """
    for src in srcs:
        if not os.path.isdir(src):
            print(f"Source '{src}' is not a directory", file=sys.stderr)
            sys.exit(ESRC_NOT_DIR)
        if not os.access(src, os.R_OK | os.X_OK):
            print(f"No access to source directory '{src}'", file=sys.stderr)
            sys.exit(ESRC_NO_ACCESS)

    if not os.path.exists(dest):
        try:
            os.mkdir(dest)
        except OSError as err:
            print(f"Error creating destination directory '{dest}': {err}", file=sys.stderr)
            sys.exit(EDEST_CREATE)

    if os.path.isfile(dest):
        print(f"Destination '{dest}' already exists and is a file", file=sys.stderr)
        sys.exit(EDEST_IS_FILE)

    if os.path.isdir(dest) and not os.access(dest, os.W_OK | os.X_OK):
        print(f"Destination directory '{dest}' not writable", file=sys.stderr)
        sys.exit(EDEST_NO_ACCESS)


def _create_level_entries(cwd, max_entries, files_pct):
    """
    Helper for testing purpose

    It will create "max_entries" entries in "cwd" with
    files_pct percent of files. The rest will be directories
    """
    dirs = list()
    files_nr = 0
    level_entries = random.randint(0, max_entries)

    for _ in range(level_entries):
        if random.randint(1, 100) <= files_pct:
            fhandle, _ = tempfile.mkstemp(dir=cwd)
            os.close(fhandle)
            files_nr += 1
        else:
            dirname = tempfile.mkdtemp(dir=cwd)
            dirs.append(dirname)

    return files_nr + len(dirs), dirs


def _create_fake_tree(cwd, total_entries, max_entries_per_level, max_depth, files_pct):
    """
    Helper for testing purpose

    This function will create a tree of 'total_entries' files and dirs in cwd, trying
    not to put more that "max_entries_per_level" entries at each level and if possible
    not to exceed a "max_depth" depth.

    The ratio of files/dirs is controlled with "files_pct". For example, if files_pct is "90",
    this function will create a tree with 90% of files and 10% of directories
    """
    dir_queue = list()
    dir_queue.append(cwd)
    curr_entries_number = 0

    root_len = len(cwd)

    while curr_entries_number < total_entries:
        if len(dir_queue) == 0:
            dir_queue.append(cwd)

        cur = dir_queue.pop()

        if cur[root_len:].count(os.sep) >= max_depth:
            continue

        entries_to_create = total_entries - curr_entries_number
        max_entries = entries_to_create if entries_to_create < max_entries_per_level else max_entries_per_level

        entries, dirs = _create_level_entries(cur, max_entries, files_pct)
        curr_entries_number += entries
        dir_queue.extend(dirs)

    return curr_entries_number


def _compare_trees(first, second):
    """
    Helper for testing purpose

    This function takes two paths, generate a listing.
    and compare them. The goal is to determine if two trees are "equal". See note.

    Note: since os.walk herits the behaviour of os.listdir, we can have different
    walk listing order for the same tree. We need to sort it before comparing anything.
    It does not scale. Ideally, the file listings would have been written in temporaries
    files and then merge-sorted (like sort(1)). Use only on tree with reasonnable size
    """
    first_list = second_list = list()
    for _, cur in crawl(first, relative=True):
        first_list.append(cur)

    first_list.sort()

    for _, cur in crawl(second, relative=True):
        second_list.append(cur)

    second_list.sort()

    return first_list == second_list


def selftest():
    """
    Run tests from the tests directory
    """
    import subprocess
    import sys

    test_runner_path = os.path.join(os.path.dirname(__file__), 'tests', 'run_tests.py')
    if os.path.exists(test_runner_path):
        subprocess.call([sys.executable, test_runner_path])
    else:
        print("Test runner not found at:", test_runner_path, file=sys.stderr)
        print("Please run tests manually from the tests directory", file=sys.stderr)


def main(cmdline):
    options, srcs, dest = parse_cmdline(cmdline)

    if options.version:
        print(f"{VERSION}", file=sys.stdout)
        sys.exit(0)

    if options.selftest:
        selftest()
        sys.exit(0)

    _check_executables()
    _check_srcs_dest(srcs, dest)
    _check_rsync_options(options.rsync)
    return msrsync(options, srcs, dest)


if __name__ == '__main__':
    try:
        sys.exit(main(sys.argv))
    except KeyboardInterrupt:
        print("\nInterrupted by user", file=sys.stderr)
        sys.exit(EMSRSYNC_INTERRUPTED)
    except SystemExit:
        raise
