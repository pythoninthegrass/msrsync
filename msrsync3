#!/usr/bin/env python

# Copyright 2017 Jean-Baptiste Denis <jbd@jbdenis.net>
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License version 3, as published
# by the Free Software Foundation.
# This file includes a copy of the BSD licensed options.py file from the bup project

VERSION = '20250809'
DEFAULT_RSYNC_OPTIONS = "-aS --numeric-ids"
MSRSYNC_OPTSPEC = f"""
msrsync [options] [--rsync "rsync-options-string"] SRCDIR [SRCDIR2...] DESTDIR
msrsync --selftest
--
 msrsync options:
p,processes=   number of rsync processes to use [1]
f,files=       limit buckets to <files> files number [1000]
s,size=        limit partitions to BYTES size (1024 suffixes: K, M, G, T, P, E, Z, Y) [1G]
b,buckets=     where to put the buckets files (default: auto temporary directory)
k,keep         do not remove buckets directory at the end
j,show         show bucket directory
P,progress     show progress
stats          show additional stats
d,dry-run      do not run rsync processes
v,version      print version
 rsync options:
r,rsync=       MUST be last option. rsync options as a quoted string ["{DEFAULT_RSYNC_OPTIONS}"]. The "--from0 --files-from=... --quiet --verbose --stats --log-file=..." options will ALWAYS be added, no matter what. Be aware that this will affect all rsync *from/filter files if you want to use them. See rsync(1) manpage for details.
 self-test options:
t,selftest     run the integrated unit and functional tests
"""

import contextlib
import getopt
import gzip
import itertools
import multiprocessing
import os
import random
import re
import shlex
import shutil
import signal
import struct
import subprocess
import sys
import tempfile
import textwrap
import threading
import time
import timeit
import traceback
from multiprocessing.managers import SyncManager

RSYNC_EXE = None
(
    EOPTION_PARSER,
    EBUCKET_DIR_NOEXIST,
    EBUCKET_DIR_PERMS,
    EBUCKET_DIR_OSERROR,
    EBUCKET_FILE_CREATE,
    EBIN_NOTFOUND,
    ESRC_NOT_DIR,
    ESRC_NO_ACCESS,
    EDEST_NO_ACCESS,
    EDEST_NOT_DIR,
    ERSYNC_OPTIONS_CHECK,
    ERSYNC_TOO_LONG,
    ERSYNC_JOB,
    ERSYNC_OK,
    EDEST_IS_FILE,
    EDEST_CREATE,
    ENEED_ROOT,
    EMSRSYNC_INTERRUPTED,
) = (97, 11, 12, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26)
(TYPE_RSYNC, TYPE_RSYNC_SENTINEL, MSG_STDERR, MSG_STDOUT, MSG_PROGRESS) = (0, 1, 10, 11, 12)
G_MESSAGES_QUEUE = None


def _invert(v, invert):
    return not v if invert else v


def _remove_negative_kv(k, v):
    return (k[3:], not v) if k.startswith(('no-', 'no_')) else (k, v)


class OptDict:
    def __init__(self, aliases):
        self._opts, self._aliases = {}, aliases

    def _unalias(self, k):
        k, reinvert = _remove_negative_kv(k, False)
        k, invert = self._aliases[k]
        return k, invert ^ reinvert

    def __setitem__(self, k, v):
        k, invert = self._unalias(k)
        self._opts[k] = _invert(v, invert)

    def __getitem__(self, k):
        k, invert = self._unalias(k)
        return _invert(self._opts[k], invert)

    def __getattr__(self, k):
        return self[k] if not k.startswith('_') else (_ for _ in ()).throw(AttributeError(k))


def _default_onabort(msg):
    sys.exit(97)


def _intify(v):
    try:
        vv = int(v or '')
        return vv if str(vv) == v else v
    except ValueError:
        return v


def _atoi(v):
    try:
        return int(v or 0)
    except ValueError:
        return 0


def _tty_width():
    try:
        import fcntl
        import termios

        s = fcntl.ioctl(sys.stderr.fileno(), termios.TIOCGWINSZ, struct.pack("HHHH", 0, 0, 0, 0))
        return struct.unpack('HHHH', s)[1] or 70
    except (OSError, ImportError, AttributeError):
        return _atoi(os.environ.get('WIDTH')) or 70


class Options:
    def __init__(self, optspec, optfunc=getopt.gnu_getopt, onabort=_default_onabort):
        self.optspec, self._onabort, self.optfunc = optspec, onabort, optfunc
        self._aliases, self._shortopts, self._longopts, self._hasparms, self._defaults = {}, 'h?', ['help', 'usage'], {}, {}
        self._usagestr = self._gen_usage()

    def _gen_usage(self):
        out, lines = [], self.optspec.strip().split('\n')[::-1]
        first_syn = True
        while lines:
            line = lines.pop()
            if line == '--':
                break
            out.append(f"{'usage' if first_syn else '   or'}: {line}\n")
            first_syn = False
        out.append('\n')
        last_was_option = False
        while lines:
            line = lines.pop()
            if line.startswith(' '):
                out.append(f"{('\n' if last_was_option else '') + line.lstrip()}\n")
                last_was_option = False
            elif line:
                flags, extra = (line + ' ').split(' ', 1)
                extra = extra.strip()
                has_parm = 1 if flags.endswith('=') else 0
                if has_parm:
                    flags = flags[:-1]
                g = re.search(r'\[([^\]]*)\]$', extra)
                defval = _intify(g.group(1)) if g else None
                flagl, flagl_nice = flags.split(','), []
                flag_main, invert_main = _remove_negative_kv(flagl[0], False)
                self._defaults[flag_main] = _invert(defval, invert_main)
                for _f in flagl:
                    f, invert = _remove_negative_kv(_f, 0)
                    self._aliases[f] = (flag_main, invert_main ^ invert)
                    self._hasparms[f] = has_parm
                    if f == '#':
                        self._shortopts += '0123456789'
                        flagl_nice.append('-#')
                    elif len(f) == 1:
                        self._shortopts += f + (':' if has_parm else '')
                        flagl_nice.append('-' + f)
                    else:
                        f_nice = re.sub(r'\W', '_', f)
                        self._aliases[f_nice] = (flag_main, invert_main ^ invert)
                        self._longopts.extend([f + ('=' if has_parm else ''), 'no-' + f])
                        flagl_nice.append('--' + _f)
                flags_nice = ', '.join(flagl_nice) + (' ...' if has_parm else '')
                prefix = f'    {flags_nice:<20}  '
                out.append(
                    '\n'.join(textwrap.wrap(extra, width=_tty_width(), initial_indent=prefix, subsequent_indent=' ' * 28)) + '\n'
                )
                last_was_option = True
            else:
                out.append('\n')
                last_was_option = False
        return ''.join(out).rstrip() + '\n'

    def usage(self, msg=""):
        sys.stderr.write(self._usagestr)
        msg and sys.stderr.write(msg)
        e = self._onabort and self._onabort(msg) or None
        e and (_ for _ in ()).throw(e)

    def fatal(self, msg):
        return self.usage(f'\nerror: {msg}\n')

    def parse(self, args):
        try:
            flags, extra = self.optfunc(args, self._shortopts, self._longopts)
        except getopt.GetoptError as e:
            self.fatal(e)
        opt = OptDict(aliases=self._aliases)
        for k, v in self._defaults.items():
            opt[k] = v
        for k, v in flags:
            k = k.lstrip('-')
            if k in ('h', '?', 'help', 'usage'):
                self.usage()
            if self._aliases.get('#') and k in '0123456789':
                v, k = int(k), self._aliases['#'][0]
                opt['#'] = v
            else:
                k, invert = opt._unalias(k)
                v = (opt._opts.get(k) or 0) + 1 if not self._hasparms[k] else _intify(v)
                opt[k] = _invert(v, invert)
        return opt, flags, extra


def print_message(message, output=MSG_STDOUT):
    G_MESSAGES_QUEUE.put({"type": output, "message": message})


def print_update(data):
    sys.stdout.write("\r\x1b[K" + str(data))
    sys.stdout.flush()


class BucketError(RuntimeError):
    pass


def get_human_size(num, power="B"):
    powers = ["B", "K", "M", "G", "T", "P", "E", "Z", "Y"]
    while num >= 1000:
        num /= 1024.0
        power = powers[powers.index(power) + 1]
    return f"{num:.1f} {power}"


def human_size(value):
    if value.isdigit():
        return int(value)
    if not value[:-1].isdigit():
        return None
    m2s = {'K': 1024, 'M': 1024**2, 'G': 1024**3, 'T': 1024**4, 'P': 1024**5, 'E': 1024**6, 'Z': 1024**7, 'Y': 1024**8}
    size, multiple = int(value[:-1]), value[-1]
    return size * m2s[multiple] if multiple in m2s else None


def crawl(path, relative=False):
    def onerror(oserror):
        print_message(f"msrsync crawl: {oserror}", MSG_STDERR)

    root_size = len(path) if relative else 0
    for root, dirs, files in os.walk(path, onerror=onerror):
        if not dirs and not files:
            try:
                yield os.lstat(root).st_size, root[root_size:]
            except OSError as err:
                print_message(f"msrsync crawl: {err}", MSG_STDERR)
                continue
        dir_links = [d for d in dirs if os.path.islink(os.path.join(root, d))]
        for name in itertools.chain(files, dir_links):
            try:
                yield os.lstat(os.path.join(root, name)).st_size, os.path.join(root, name)[root_size:]
            except OSError as err:
                print_message(f"msrsync crawl: {err}", MSG_STDERR)


def buckets(path, filesnr, size):
    bucket_files_nr = bucket_size = 0
    bucket, base = [], os.path.split(path)[1]
    for fsize, rpath in crawl(path, relative=True):
        bucket.append(os.path.join(base, rpath.lstrip(os.sep)))
        bucket_files_nr += 1
        bucket_size += fsize
        if bucket_size >= size or bucket_files_nr >= filesnr:
            yield bucket_files_nr, bucket_size, bucket
            bucket_size = bucket_files_nr = 0
            bucket = []
    if bucket_files_nr > 0:
        yield bucket_files_nr, bucket_size, bucket


def _valid_rsync_options(options, rsync_opts):
    for opt in rsync_opts.split():
        if opt.startswith("--delete"):
            options.fatal("Cannot use --delete option type with msrsync. It would lead to disaster :)")


def parse_cmdline(cmdline_argv):
    options = Options(MSRSYNC_OPTSPEC)
    if "-r" in cmdline_argv:
        cmdline_argv[cmdline_argv.index("-r")] = "--rsync"
    if "--rsync" in cmdline_argv:
        idx = cmdline_argv.index("--rsync")
        opt, _, extra = options.parse(cmdline_argv[1:idx])
        if len(cmdline_argv[idx:]) < 4:
            options.fatal('You must provide a source, a destination and eventually rsync options with --rsync')
        opt.rsync = opt.r = cmdline_argv[idx + 1]
        _valid_rsync_options(options, opt.rsync)
        srcs, dest = cmdline_argv[idx + 2 : -1], cmdline_argv[-1]
    else:
        opt, _, extra = options.parse(cmdline_argv[1:])
        if opt.selftest or opt.version:
            return opt, [], ""
        opt.rsync = opt.r = DEFAULT_RSYNC_OPTIONS
        if not extra or len(extra) < 2:
            options.fatal('You must provide a source and a destination')
        srcs, dest = extra[:-1], extra[-1]
    size = human_size(str(opt.size))
    if not size:
        options.fatal(f"'{opt.size}' does not look like a valid size value")
    try:
        opt.files = opt.f = int(opt.f)
    except ValueError:
        options.fatal(f"'{opt.f}' does not look like a valid files number value")
    opt.size = opt.s = size
    opt.compress = False
    return opt, srcs, dest


def rmtree_onerror(func, path, exc_info):
    print("Error removing", path, file=sys.stderr)


def write_bucket(filename, bucket, compress=False):
    try:
        fileno, path = filename
        if not compress:
            with os.fdopen(fileno, 'wb') as bfile:
                for entry in bucket:
                    bfile.write((entry + '\0').encode('utf-8', 'surrogateescape'))
        else:
            os.close(fileno)
            with gzip.open(path, 'wb') as bfile:
                for entry in bucket:
                    bfile.write(entry.encode('utf-8', 'surrogateescape'))
    except OSError as err:
        raise BucketError(f"Cannot write bucket file {path}: {err}") from err


def consume_queue(jobs_queue):
    while True:
        item = jobs_queue.get()
        if item is StopIteration:
            return
        yield item


def kill_proc(proc, timeout):
    timeout["value"] = True
    with contextlib.suppress(OSError, ProcessLookupError):
        try:
            os.killpg(proc.pid, signal.SIGTERM)
            time.sleep(0.1)
            os.killpg(proc.pid, signal.SIGKILL)
        except (OSError, ProcessLookupError):
            proc.terminate()
            time.sleep(0.1)
            proc.kill()


def run(cmd, capture_stdout=False, capture_stderr=False, timeout_sec=sys.maxsize):
    return run_tracked(cmd, None, capture_stdout, capture_stderr, timeout_sec)


def run_tracked(cmd, proc_tracker, capture_stdout=False, capture_stderr=False, timeout_sec=sys.maxsize):
    try:
        proc = subprocess.Popen(
            shlex.split(cmd),
            stdout=subprocess.PIPE if capture_stdout else None,
            stderr=subprocess.PIPE if capture_stderr else None,
            preexec_fn=os.setpgrp,
        )
        if proc_tracker:
            proc_tracker["proc"] = proc
        timeout = {"value": False}
        timer = threading.Timer(timeout_sec, kill_proc, [proc, timeout])
        starttime = time.time()
        timer.start()
        stdout, stderr = proc.communicate()
        timer.cancel()
        elapsed = time.time() - starttime
        if proc_tracker:
            proc_tracker["proc"] = None
        return (
            proc.returncode,
            (stdout or b"").decode('utf-8', 'replace'),
            (stderr or b"").decode('utf-8', 'replace'),
            timeout["value"],
            elapsed,
        )
    except OSError as err:
        return -1, "", f"Cannot launch {cmd}: {err}", False, 0
    except KeyboardInterrupt:
        if 'timer' in vars():
            timer.cancel()
        if 'proc' in vars():
            [stream.close() for stream in [proc.stdout, proc.stderr] if stream]
        if 'proc' in vars():
            try:
                os.killpg(proc.pid, signal.SIGTERM)
                time.sleep(0.1)
                os.killpg(proc.pid, signal.SIGKILL)
            except (OSError, ProcessLookupError):
                proc.terminate()
                time.sleep(0.1)
                proc.kill()
            proc.wait()
        if proc_tracker:
            proc_tracker["proc"] = None
        return EMSRSYNC_INTERRUPTED, "", "Interrupted", False, 0


def which(program):
    def is_exe(fpath):
        return os.path.isfile(fpath) and os.access(fpath, os.X_OK)

    fpath, _ = os.path.split(program)
    if fpath:
        return program if is_exe(program) else None
    for path in os.environ["PATH"].split(os.pathsep):
        exe_file = os.path.join(path, program)
        if is_exe(exe_file):
            return exe_file
    return None


def _check_rsync_options(options):
    rsync_cmd = None
    try:
        src, dst = tempfile.mkdtemp(), tempfile.mkdtemp()
        rsync_log_fd, rsync_log = tempfile.mkstemp()
        rsync_cmd = f"{RSYNC_EXE} {options + ' --quiet --stats --verbose --from0 --log-file ' + rsync_log} {src + os.sep} {dst}"
        ret, _, stderr, timeout, _ = run(rsync_cmd, timeout_sec=60)
        if timeout:
            print(f'Error during rsync options check command "{rsync_cmd}": took more than 60 seconds !', file=sys.stderr)
            sys.exit(ERSYNC_OPTIONS_CHECK)
        elif ret != 0:
            print(f'Error during rsync options check command "{rsync_cmd}": {2 * os.linesep + stderr}', file=sys.stderr)
            sys.exit(ERSYNC_OPTIONS_CHECK)
    except OSError as err:
        msg = (
            f'Error during rsync options check command "{rsync_cmd}": {2 * os.linesep + str(err)}'
            if rsync_cmd
            else f'Error during rsync options check ("{options}"): {2 * os.linesep + str(err)}'
        )
        print(msg, file=sys.stderr)
        sys.exit(ERSYNC_OPTIONS_CHECK)
    finally:
        try:
            os.rmdir(src)
            os.rmdir(dst)
            os.close(rsync_log_fd)
            os.remove(rsync_log)
        except OSError:
            pass


def run_rsync(files_from, rsync_opts, src, dest, timeout=3600 * 24 * 7):
    return run_rsync_tracked(files_from, rsync_opts, src, dest, None, timeout)


def run_rsync_tracked(files_from, rsync_opts, src, dest, proc_tracker, timeout=3600 * 24 * 7):
    rsync_log = files_from + '.log'
    rsync_cmd = f'{RSYNC_EXE} {rsync_opts} --quiet --verbose --stats --from0 --files-from={files_from} --log-file={rsync_log} "{src}" "{dest}"'
    rsync_result = dict(rcode=-1, msg=None, cmdline=rsync_cmd, log=rsync_log)
    try:
        ret, _, _, timeout, elapsed = run_tracked(rsync_cmd, proc_tracker, timeout_sec=timeout)
        rsync_result.update(rcode=ret, elapsed=elapsed)
        if timeout:
            rsync_result["errcode"] = ERSYNC_TOO_LONG
        elif ret != 0:
            rsync_result["errcode"] = ERSYNC_JOB
    except OSError as err:
        rsync_result.update(errcode=ERSYNC_JOB, msg=str(err))
    return rsync_result


def rsync_worker(jobs_queue, monitor_queue, options, dest, rsync_exe):
    current_rsync_proc = {"proc": None}

    def worker_signal_handler(signum, frame):
        if current_rsync_proc["proc"]:
            try:
                os.killpg(current_rsync_proc["proc"].pid, signal.SIGTERM)
                time.sleep(0.1)
                os.killpg(current_rsync_proc["proc"].pid, signal.SIGKILL)
            except (OSError, ProcessLookupError):
                current_rsync_proc["proc"].terminate()
                current_rsync_proc["proc"].kill()
        sys.exit(1)

    signal.signal(signal.SIGINT, worker_signal_handler)
    signal.signal(signal.SIGTERM, worker_signal_handler)
    global RSYNC_EXE
    RSYNC_EXE = rsync_exe
    try:
        for src, files_from, bucket_files_nr, bucket_size in consume_queue(jobs_queue):
            rsync_result = (
                dict(rcode=0, elapsed=0, errcode=0, msg='')
                if options.dry_run
                else run_rsync_tracked(files_from, options.rsync, src, dest, current_rsync_proc)
            )
            monitor_queue.put(
                {
                    "type": TYPE_RSYNC,
                    "rsync_result": rsync_result,
                    "size": bucket_size,
                    "files_nr": bucket_files_nr,
                    "jq_size": jobs_queue.qsize(),
                }
            )
    except (KeyboardInterrupt, SystemExit):
        pass
    finally:
        try:
            jobs_queue.put(StopIteration)
            monitor_queue.put({"type": TYPE_RSYNC_SENTINEL, "pid": os.getpid()})
        except (OSError, BrokenPipeError, ConnectionResetError, EOFError):
            pass


def handle_rsync_error_result(rsync_result, messages_queue):
    msg = rsync_result["msg"] or ''
    try:
        match rsync_result["errcode"]:
            case code if code == ERSYNC_TOO_LONG:
                messages_queue.put(
                    {
                        "type": MSG_STDERR,
                        "message": f"rsync command took too long and has been killed (see '{rsync_result['log']}' rsync log file): {msg}\n{rsync_result['cmdline']}",
                    }
                )
            case code if code == ERSYNC_JOB:
                messages_queue.put(
                    {
                        "type": MSG_STDERR,
                        "message": f"errors during rsync command (see '{rsync_result['log']}' rsync log file): {msg}\n{rsync_result['cmdline']}",
                    }
                )
            case _:
                messages_queue.put({"type": MSG_STDERR, "message": f"unknown rsync_result status: {rsync_result}"})
    except (OSError, BrokenPipeError, ConnectionResetError, EOFError):
        pass


def rsync_monitor_worker(
    monitor_queue, nb_rsync_processes, total_size, total_files_nr, crawl_time, total_time, options, messages_queue
):
    signal.signal(signal.SIGINT, signal.SIG_DFL)
    signal.signal(signal.SIGTERM, signal.SIG_DFL)
    current_size = current_files_nr = current_elapsed = rsync_runtime = rsync_workers_stops = buckets_nr = rsync_errors = (
        entries_per_second
    ) = bytes_per_second = 0
    try:
        start = timeit.default_timer()
        for result in consume_queue(monitor_queue):
            if result["type"] == TYPE_RSYNC_SENTINEL:
                rsync_workers_stops += 1
                continue
            if result["type"] != TYPE_RSYNC:
                with contextlib.suppress(OSError, BrokenPipeError, ConnectionResetError, EOFError):
                    messages_queue.put(
                        {
                            "type": MSG_STDERR,
                            "message": f"rsync_monitor_worker process received an incompatile type message: {result}",
                        }
                    )
                continue
            rsync_result = result["rsync_result"]
            if rsync_result["rcode"] != 0:
                rsync_errors += 1
                handle_rsync_error_result(rsync_result, messages_queue)
                continue
            buckets_nr += 1
            current_size += result["size"]
            current_files_nr += result["files_nr"]
            rsync_runtime += result["rsync_result"]["elapsed"]
            current_elapsed = timeit.default_timer() - start
            bytes_per_second = current_size / current_elapsed if current_elapsed > 0 else 0
            entries_per_second = current_files_nr / current_elapsed if current_elapsed > 0 else 0
            if options.progress:
                with contextlib.suppress(OSError, BrokenPipeError, ConnectionResetError, EOFError):
                    messages_queue.put(
                        {
                            "type": MSG_PROGRESS,
                            "message": f"[{current_files_nr}/{total_files_nr.value} entries] [{get_human_size(current_size)}/{get_human_size(total_size.value)} transferred] [{entries_per_second} entries/s] [{get_human_size(bytes_per_second)}/s bw] [monq {monitor_queue.qsize()}] [jq {result['jq_size']}]",
                        }
                    )
        if rsync_errors > 0:
            with contextlib.suppress(OSError, BrokenPipeError, ConnectionResetError, EOFError):
                messages_queue.put(
                    {
                        "type": MSG_STDERR,
                        "message": "\nmsrsync error: somes files/attr were not transferred (see previous errors)",
                    }
                )
        stats = dict(
            errors=rsync_errors,
            total_size=total_size.value,
            total_entries=total_files_nr.value,
            buckets_nr=buckets_nr,
            bytes_per_second=bytes_per_second,
            entries_per_second=entries_per_second,
            rsync_workers=nb_rsync_processes,
            rsync_runtime=rsync_runtime,
            crawl_time=crawl_time.value,
            total_time=total_time.value,
        )
        with contextlib.suppress(OSError, BrokenPipeError, ConnectionResetError, EOFError):
            monitor_queue.put(stats)
    except (KeyboardInterrupt, SystemExit):
        pass


def messages_worker(options, messages_queue):
    signal.signal(signal.SIGINT, signal.SIG_DFL)
    signal.signal(signal.SIGTERM, signal.SIG_DFL)
    last_msg_type = cur_msg_type = None
    try:
        for result in consume_queue(messages_queue):
            newline = os.linesep if last_msg_type == MSG_PROGRESS else ''
            cur_msg_type = result["type"]
            match cur_msg_type:
                case msg_type if msg_type == MSG_PROGRESS:
                    print_update(result["message"])
                case msg_type if msg_type == MSG_STDOUT:
                    print(newline + result["message"], file=sys.stdout)
                case msg_type if msg_type == MSG_STDERR:
                    print(newline + result["message"], file=sys.stderr)
                case _:
                    print(newline + f"Unknown message type '{cur_msg_type}': {result}", file=sys.stderr)
            last_msg_type = cur_msg_type
    except (KeyboardInterrupt, SystemExit):
        pass
    finally:
        if last_msg_type == MSG_PROGRESS:
            print('', file=sys.stdout)


def start_rsync_workers(jobs_queue, monitor_queue, options, dest):
    processes = [
        multiprocessing.Process(target=rsync_worker, args=(jobs_queue, monitor_queue, options, dest, RSYNC_EXE))
        for _ in range(options.processes)
    ]
    for p in processes:
        p.start()
    return processes


def start_rsync_monitor_worker(
    monitor_queue, nb_rsync_processes, total_size, total_files_nr, crawl_time, total_time, options, messages_queue
):
    proc = multiprocessing.Process(
        target=rsync_monitor_worker,
        args=(monitor_queue, nb_rsync_processes, total_size, total_files_nr, crawl_time, total_time, options, messages_queue),
    )
    proc.start()
    return proc


def start_messages_worker(options, messages_queue):
    proc = multiprocessing.Process(target=messages_worker, args=(options, messages_queue))
    proc.start()
    return proc


def multiprocess_mgr_init():
    signal.signal(signal.SIGINT, signal.SIG_IGN)


def show_stats(msrsync_stat):
    s = msrsync_stat
    status = "SUCCESS" if s["errors"] == 0 else f"FAILURE, {s['errors']} rsync processe(s) had errors"
    buckets_nr = s["buckets_nr"]
    print("Status:", status)
    print("Working directory:", os.getcwd())
    print("Command line:", " ".join(sys.argv))
    print(f"Total size: {get_human_size(s['total_size'])}")
    print(f"Total entries: {s['total_entries']}")
    print(f"Buckets number: {buckets_nr}")
    if buckets_nr > 0:
        print(f"Mean entries per bucket: {int((s['total_entries'] * 1.0) / buckets_nr)}")
        print(f"Mean size per bucket: {get_human_size((s['total_size'] * 1.0) / buckets_nr)}")
    print(f"Entries per second: {s['entries_per_second']:.0f}")
    print(f"Speed: {get_human_size(s['bytes_per_second'])}/s")
    print(f"Rsync workers: {s['rsync_workers']}")
    print(f"Total rsync's processes ({buckets_nr}) cumulative runtime: {s['rsync_runtime']:.1f}s")
    print(f"Crawl time: {s['crawl_time']:.1f}s ({100 * s['crawl_time'] / s['total_time']:.1f}% of total runtime)")
    print(f"Total time: {s['total_time']:.1f}s")


def msrsync(options, srcs, dest):
    global G_MESSAGES_QUEUE
    try:
        if not options.buckets:
            options.buckets = tempfile.mkdtemp(prefix="msrsync-")
        else:
            if not os.path.exists(options.buckets):
                print(options.buckets, "bucket directory does not exist.", file=sys.stderr)
                sys.exit(EBUCKET_DIR_NOEXIST)
            if not os.access(options.buckets, os.W_OK):
                print(options.buckets, "bucket directory is not writable.", file=sys.stderr)
                sys.exit(EBUCKET_DIR_PERMS)
            options.buckets = tempfile.mkdtemp(prefix="msrsync-", dir=options.buckets)
    except OSError as err:
        print(f'Error with bucket directory creation: "{err}"', file=sys.stderr)
        sys.exit(EBUCKET_DIR_OSERROR)
    if options.show:
        print("buckets dir is", options.buckets)
    manager = SyncManager()
    manager.start()

    def signal_handler(signum, frame):
        raise KeyboardInterrupt()

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    total_size, total_files_nr, crawl_time, total_time = (
        manager.Value('i', 0),
        manager.Value('i', 0),
        manager.Value('f', 0),
        manager.Value('f', 0),
    )
    monitor_queue, jobs_queue = manager.Queue(), manager.Queue()
    G_MESSAGES_QUEUE = manager.Queue()
    rsync_workers_procs = start_rsync_workers(jobs_queue, monitor_queue, options, dest)
    rsync_monitor_worker_proc = start_rsync_monitor_worker(
        monitor_queue, options.processes, total_size, total_files_nr, crawl_time, total_time, options, G_MESSAGES_QUEUE
    )
    messages_worker_proc = start_messages_worker(options, G_MESSAGES_QUEUE)
    crawl_start = timeit.default_timer()
    try:
        total_size.value = bucket_nr = 0
        for src in srcs:
            head, tail = os.path.split(src)
            src_base = os.getcwd() if head == '' else head
            for bucket_files_nr, bucket_size, bucket in buckets(src, options.files, options.s):
                total_size.value += bucket_size
                total_files_nr.value += bucket_files_nr
                bucket.sort()
                d1s = str(bucket_nr / 1024).zfill(8)
                try:
                    tdir = os.path.join(options.buckets, d1s[:4], d1s[4:])
                    os.path.exists(tdir) or os.makedirs(tdir)
                    fileno, filename = tempfile.mkstemp(dir=tdir)
                except OSError as err:
                    print_message(f'msrsync scan: cannot create temporary bucket file: "{err}"', MSG_STDERR)
                    continue
                write_bucket((fileno, filename), bucket, options.compress)
                bucket_nr += 1
                jobs_queue.put((src_base, filename, bucket_files_nr, bucket_size))
        crawl_time.value = timeit.default_timer() - crawl_start
        jobs_queue.put(StopIteration)
        for worker in rsync_workers_procs:
            worker.join()
        total_time.value = timeit.default_timer() - crawl_start
        monitor_queue.put(StopIteration)
        rsync_monitor_worker_proc.join()
        G_MESSAGES_QUEUE.put(StopIteration)
        messages_worker_proc.join()
        run_stats = monitor_queue.get()
        if options.stats:
            show_stats(run_stats)
        return run_stats["errors"]
    except (KeyboardInterrupt, SystemExit):
        print("\nInterrupted! Terminating processes...", file=sys.stderr)
        sys.stdout.flush()
        sys.stderr.flush()
        original_stderr = sys.stderr
        with open(os.devnull, 'w') as devnull:
            sys.stderr = devnull
            try:
                for worker in rsync_workers_procs:
                    worker.terminate()
                rsync_monitor_worker_proc.terminate()
                messages_worker_proc.terminate()
                for worker in rsync_workers_procs:
                    worker.join(timeout=2.0)
                    worker.is_alive() and worker.kill()
                rsync_monitor_worker_proc.join(timeout=2.0)
                rsync_monitor_worker_proc.is_alive() and rsync_monitor_worker_proc.kill()
                messages_worker_proc.join(timeout=2.0)
                messages_worker_proc.is_alive() and messages_worker_proc.kill()
            finally:
                sys.stderr = original_stderr
        return EMSRSYNC_INTERRUPTED
    except BucketError as err:
        print(err, file=sys.stderr)
    except Exception:
        print("Uncaught exception:" + os.linesep + traceback.format_exc(), file=sys.stderr)
    finally:
        manager.shutdown()
        options.buckets is not None and not options.keep and shutil.rmtree(options.buckets, onerror=rmtree_onerror)


def _check_executables():
    global RSYNC_EXE
    prog = which("rsync")
    if not prog:
        print("Cannot find 'rsync' executable in PATH.", file=sys.stderr)
        sys.exit(EBIN_NOTFOUND)
    RSYNC_EXE = prog


def _check_srcs_dest(srcs, dest):
    for src in srcs:
        if not os.path.isdir(src):
            print(f"Source '{src}' is not a directory", file=sys.stderr)
            sys.exit(ESRC_NOT_DIR)
        if not os.access(src, os.R_OK | os.X_OK):
            print(f"No access to source directory '{src}'", file=sys.stderr)
            sys.exit(ESRC_NO_ACCESS)
    if not os.path.exists(dest):
        try:
            os.mkdir(dest)
        except OSError as err:
            print(f"Error creating destination directory '{dest}': {err}", file=sys.stderr)
            sys.exit(EDEST_CREATE)
    if os.path.isfile(dest):
        print(f"Destination '{dest}' already exists and is a file", file=sys.stderr)
        sys.exit(EDEST_IS_FILE)
    if os.path.isdir(dest) and not os.access(dest, os.W_OK | os.X_OK):
        print(f"Destination directory '{dest}' not writable", file=sys.stderr)
        sys.exit(EDEST_NO_ACCESS)


def _create_level_entries(cwd, max_entries, files_pct):
    dirs, files_nr = [], 0
    for _ in range(random.randint(0, max_entries)):
        if random.randint(1, 100) <= files_pct:
            fhandle, _ = tempfile.mkstemp(dir=cwd)
            os.close(fhandle)
            files_nr += 1
        else:
            dirs.append(tempfile.mkdtemp(dir=cwd))
    return files_nr + len(dirs), dirs


def _create_fake_tree(cwd, total_entries, max_entries_per_level, max_depth, files_pct):
    dir_queue, curr_entries_number, root_len = [cwd], 0, len(cwd)
    while curr_entries_number < total_entries:
        if not dir_queue:
            dir_queue.append(cwd)
        cur = dir_queue.pop()
        if cur[root_len:].count(os.sep) >= max_depth:
            continue
        entries_to_create = total_entries - curr_entries_number
        max_entries = entries_to_create if entries_to_create < max_entries_per_level else max_entries_per_level
        entries, dirs = _create_level_entries(cur, max_entries, files_pct)
        curr_entries_number += entries
        dir_queue.extend(dirs)
    return curr_entries_number


def _compare_trees(first, second):
    first_list = sorted([cur for _, cur in crawl(first, relative=True)])
    second_list = sorted([cur for _, cur in crawl(second, relative=True)])
    return first_list == second_list


def selftest():
    test_runner_path = os.path.join(os.path.dirname(__file__), 'tests', 'run_tests.py')
    if os.path.exists(test_runner_path):
        subprocess.call([sys.executable, test_runner_path])
    else:
        print("Test runner not found at:", test_runner_path, file=sys.stderr)
        print("Please run tests manually from the tests directory", file=sys.stderr)


def main(cmdline):
    options, srcs, dest = parse_cmdline(cmdline)
    if options.version:
        print(f"{VERSION}", file=sys.stdout)
        sys.exit(0)
    if options.selftest:
        selftest()
        sys.exit(0)
    _check_executables()
    _check_srcs_dest(srcs, dest)
    _check_rsync_options(options.rsync)
    return msrsync(options, srcs, dest)


if __name__ == '__main__':
    try:
        sys.exit(main(sys.argv))
    except KeyboardInterrupt:
        print("\nInterrupted by user", file=sys.stderr)
        sys.exit(EMSRSYNC_INTERRUPTED)
    except SystemExit:
        raise
